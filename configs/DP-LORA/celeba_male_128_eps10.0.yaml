setup:
  method: dpsgd-lora
  run_type: normal
  n_gpus_per_node: 1
  n_nodes: 1
  node_rank: 0
  master_address: 127.0.0.1
  master_port: 6025
  omp_n_threads: 8
  workdir: exp/dpdm/mnist_28_eps1.0
public_data:
  name: imagenet
  num_channels: 3
  resolution: 128
  n_classes: 1000
  train_path: dataset/dpimagebench/imagenet_128
  selective:
    ratio: 1.0
sensitive_data:
  name: celeba_male_128
  num_channels: 3
  resolution: 128
  n_classes: 2
  train_path: dataset/celeba/train_128_Male.zip
  test_path: dataset/celeba/test_128_Male.zip
  fid_stats: dataset/celeba/fid_stats_128.npz
model:
  ckpt: null
  network:
    attn_resolutions:
    - 14
    - 7
    ch_mult:
    - 1
    - 2
    nf: 32
pretrain:
  log_dir: null
  autoencoder:
    config_path: ./models/DP_LDM/configs/autoencoder/autoencoder_kl_128.yaml
    n_epochs: 4
    batch_size: 64
  unet:
    config_path: ./models/DP_LDM/configs/latent-diffusion/128_4M.yaml
    n_epochs: 40
    batch_size: 64
  batch_size: 64
  cond: true
train:
  config_path: ./models/DP_LORA/configs/finetuning/128_4M.yaml
  log_dir: null
  seed: 0
  batch_size: 4096
  n_epochs: 50
  dp:
    sdq: null
    max_grad_norm: 0.001
    delta: 1e-5
    epsilon: 10.0
    max_physical_batch_size: 8192
  n_splits: 256
gen:
  data_num: 60000
  batch_size: 1000
  log_dir: null
eval:
  batch_size: 1000
  mode: val
