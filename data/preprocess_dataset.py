import functools
import gzip
import io
import json
import os
import pickle
import re
import sys
import tarfile
import zipfile
from pathlib import Path
import os
import argparse
from tqdm import tqdm
import PIL
import numpy as np
from typing import Callable, Optional, Tuple, Union
import functools
import json

import torch
import torchvision
from torchvision import transforms
from torch.utils.data import Dataset, ConcatDataset

from fld.features.InceptionFeatureExtractor import InceptionFeatureExtractor

new_cwd = os.path.dirname(os.getcwd())
sys.path.insert(0, new_cwd)
from data.stylegan3.dataset import ImageFolderDataset

import os
from PIL import Image

class COVIDxDataset(Dataset):
    def __init__(self, root_dir, split_files, transform=None):
        """
        root_dir: Dataset root directory (e.g., dataset/covidx-cxr4)
        split_files: List of files like train.txt and/or val.txt, merged in order
        transform: Image transformations (e.g., ToTensor, Resize)
        """
        self.root_dir = root_dir
        self.transform = transform
        self.data = []
        self.labels = []
        self.label_map = {'negative': 0, 'positive': 1}  # Binary classification

        # Read all split files in order
        for split_file in split_files:
            if not os.path.exists(split_file):
                print(f"Warning: {split_file} does not exist, skipping")
                continue
            # Determine subfolder based on split_file name (e.g., 'train.txt' -> 'train')
            subfolder = os.path.basename(split_file).replace('.txt', '')
            with open(split_file, 'r') as f:
                lines = f.readlines()
                for line in lines:
                    # Format: [patient_id] [filename] [class] [data_source]
                    parts = line.strip().split()
                    if len(parts) >= 4:  # Ensure at least 4 columns
                        filename = parts[1]  # Filename in 2nd column
                        label = parts[2]     # Class in 3rd column
                        img_path = os.path.join(root_dir, subfolder, filename)
                        if not os.path.exists(img_path):
                            print(f"Warning: Image not found: {img_path}")
                            continue
                        self.data.append(img_path)
                        self.labels.append(self.label_map.get(label, 0))  # Map label to integer

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img_path = self.data[idx]
        label = self.labels[idx]
        image = Image.open(img_path).convert('RGB')  # Ensure 3-channel images
        if self.transform:
            image = self.transform(image)
        return image, label

# resize images into the needed resolution
def resize_images(source_dir, target_dir, size=(32, 32)):

    for root, dirs, files in os.walk(source_dir):

        relative_path = os.path.relpath(root, source_dir)

        target_subdir = os.path.join(target_dir, relative_path)

        os.makedirs(target_subdir, exist_ok=True)

        for file in files:
            file_ext = file.lower().split('.')[-1]
            if file_ext in ['jpg', 'jpeg', 'png', 'bmp', 'gif', 'tiff']:
                source_file = os.path.join(root, file)
                target_file = os.path.join(target_subdir, file)
                with Image.open(source_file) as img:
                    img = img.convert('RGB')
                    img_resized = img.resize(size, Image.LANCZOS)
                    img_resized.save(target_file)
                # try:
                #     with Image.open(source_file) as img:
                #         img = img.convert('RGB')
                #         img_resized = img.resize(size, Image.LANCZOS)
                #         img_resized.save(target_file)
                # except Exception as e:
                #     print(f" {source_file} : {e}")


def error(msg):
    print('Error: ' + msg)
    sys.exit(1)

class target_trans(object):
    def __init__(self, attr_index):
        self.idx = attr_index
        
    def __call__(self, attrs):
        return attrs[self.idx]

    def __repr__(self):
        return self.__class__.__name__

def make_transform(
    transform: Optional[str],
    output_width: Optional[int],
    output_height: Optional[int]
) -> Callable[[np.ndarray], Optional[np.ndarray]]:
    def scale(width, height, img):
        w = img.shape[1]
        h = img.shape[0]
        if width == w and height == h:
            return img
        img = PIL.Image.fromarray(img)
        ww = width if width is not None else w
        hh = height if height is not None else h
        img = img.resize((ww, hh), PIL.Image.LANCZOS)
        return np.array(img)

    def center_crop(width, height, img):
        crop = np.min(img.shape[:2])
        img = img[(img.shape[0] - crop) // 2 : (img.shape[0] + crop) // 2, (img.shape[1] - crop) // 2 : (img.shape[1] + crop) // 2]
        try:
            img = PIL.Image.fromarray(img, 'RGB')
        except:
            print(img.shape)
        img = img.resize((width, height), PIL.Image.LANCZOS)
        return np.array(img)

    def center_crop_wide(width, height, img):
        ch = int(np.round(width * img.shape[0] / img.shape[1]))
        if img.shape[1] < width or ch < height:
            return None

        img = img[(img.shape[0] - ch) // 2 : (img.shape[0] + ch) // 2]
        img = PIL.Image.fromarray(img, 'RGB')
        img = img.resize((width, height), PIL.Image.LANCZOS)
        img = np.array(img)

        canvas = np.zeros([width, width, 3], dtype=np.uint8)
        canvas[(width - height) // 2 : (width + height) // 2, :] = img
        return canvas

    if transform is None:
        return functools.partial(scale, output_width, output_height)
    if transform == 'center-crop':
        if (output_width is None) or (output_height is None):
            error ('must specify --resolution=WxH when using ' + transform + 'transform')
        return functools.partial(center_crop, output_width, output_height)
    if transform == 'center-crop-wide':
        if (output_width is None) or (output_height is None):
            error ('must specify --resolution=WxH when using ' + transform + ' transform')
        return functools.partial(center_crop_wide, output_width, output_height)
    assert False, 'unknown transform'

def file_ext(name: Union[str, Path]) -> str:
    return str(name).split('.')[-1]

def open_dest(dest: str) -> Tuple[str, Callable[[str, Union[bytes, str]], None], Callable[[], None]]:
    dest_ext = file_ext(dest)

    if dest_ext == 'zip':
        if os.path.dirname(dest) != '':
            os.makedirs(os.path.dirname(dest), exist_ok=True)
        zf = zipfile.ZipFile(file=dest, mode='w', compression=zipfile.ZIP_STORED)
        def zip_write_bytes(fname: str, data: Union[bytes, str]):
            zf.writestr(fname, data)
        return '', zip_write_bytes, zf.close
    else:
        # If the output folder already exists, check that is is
        # empty.
        #
        # Note: creating the output directory is not strictly
        # necessary as folder_write_bytes() also mkdirs, but it's better
        # to give an error message earlier in case the dest folder
        # somehow cannot be created.
        if os.path.isdir(dest) and len(os.listdir(dest)) != 0:
            error('--dest folder must be empty')
        os.makedirs(dest, exist_ok=True)

        def folder_write_bytes(fname: str, data: Union[bytes, str]):
            os.makedirs(os.path.dirname(fname), exist_ok=True)
            with open(fname, 'wb') as fout:
                if isinstance(data, str):
                    data = data.encode('utf8')
                fout.write(data)
        return dest, folder_write_bytes, lambda: None

# preprocess and save the images
def data_save(data_loader, num_files, transform_image, archive_root_dir, save_bytes, close_dest):
    labels = []
    for idx, (image, label) in tqdm(enumerate(data_loader), total=num_files):
        idx_str = f'{idx:08d}'
        archive_fname = f'{idx_str[:5]}/img{idx_str}.png'
        
        label = int(label.long().numpy().astype('uint8')[0])

        if image.shape[1] == 1:
            image = image.repeat(1, 3, 1, 1)
        image = (image * 255)[0].permute(1, 2, 0).numpy().astype('uint8')

        # Apply crop and resize.
        img = transform_image(image)
        image = image[..., :1]

        # Transform may drop images.
        if img is None:
            continue

        # Error check to require uniform image attributes across
        # the whole dataset.
        channels = img.shape[2] if img.ndim == 3 else 1
        cur_image_attrs = {
            'width': img.shape[1],
            'height': img.shape[0],
            'channels': channels
        }

        dataset_attrs = cur_image_attrs
        width = dataset_attrs['width']
        height = dataset_attrs['height']
        if width != height:
            error(f'Image dimensions after scale and crop are required to be square.  Got {width}x{height}')
        if dataset_attrs['channels'] not in [1, 3]:
            error('Input images must be stored as RGB or grayscale')
        # if width != 2 ** int(np.floor(np.log2(width))):
        #     error('Image width/height after scale and crop are required to be power-of-two')

        # Save the image as an uncompressed PNG.
        img = PIL.Image.fromarray(img, { 1: 'L', 3: 'RGB' }[channels])
        image_bits = io.BytesIO()
        img.save(image_bits, format='png', compress_level=0, optimize=False)
        save_bytes(os.path.join(archive_root_dir, archive_fname), image_bits.getbuffer())
        labels.append([archive_fname, label] if label is not None else None)

    metadata = {
        'labels': labels if all(x is not None for x in labels) else None
    }
    save_bytes(os.path.join(archive_root_dir, 'dataset.json'), json.dumps(metadata))
    close_dest()

def get_activations(dl, model, device, max_samples):
    pred_arr = []
    total_processed = 0

    print('Starting to sample.')
    for batch in dl:
        # ignore labels
        if isinstance(batch, list):
            batch = batch[0]
        batch = batch.to(device)
        if batch.shape[1] == 1:  # if image is gray scale
            batch = batch.repeat(1, 3, 1, 1)
        elif len(batch.shape) == 3:  # if image is gray scale
            batch = batch.unsqueeze(1).repeat(1, 3, 1, 1)
        
        pred = model.get_feature_batch(batch).cpu().numpy()

        pred_arr.append(pred)
        total_processed += pred.shape[0]
        if max_samples is not None and total_processed > max_samples:
            print('Max of %d samples reached.' % max_samples)
            break

    pred_arr = np.concatenate(pred_arr, axis=0)
    if max_samples is not None:
        pred_arr = pred_arr[:max_samples]

    return pred_arr

def fid_save(fid_path, dataset, batch_size):
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'

    queue = torch.utils.data.DataLoader(
        dataset=dataset, batch_size=batch_size, pin_memory=True, num_workers=1)
    
    inception_model = InceptionFeatureExtractor()
    inception_model.model = inception_model.model.to(device)

    act = get_activations(queue, inception_model, device=device, max_samples=None)
    mu = np.mean(act, axis=0)
    sigma = np.cov(act, rowvar=False)
    np.savez(fid_path, mu=mu, sigma=sigma)

def main(config):
    for data_name in config.data_name:
        if data_name == 'celeba':
            # res_list = [32, 64, 128]
            res_list = [256]
        elif data_name == 'covidx-cxr4':
            res_list = [512]
        elif 'mnist' in data_name:
            res_list = [28]
        else:
            res_list = [32]

        for res in res_list:
            config.resolution = res
            train_name = "train_{}".format(config.resolution)
            test_name = "test_{}".format(config.resolution)
            fid_name = "fid_stats_{}".format(config.resolution)

            data_dir = os.path.join(config.data_dir, data_name)
            os.makedirs(data_dir, exist_ok=True)
            if data_name == "mnist":
                sensitive_train_set = torchvision.datasets.MNIST(root=data_dir, train=True, download=True, transform=transforms.ToTensor())
                sensitive_test_set = torchvision.datasets.MNIST(root=data_dir, train=False, download=True, transform=transforms.ToTensor())
            elif data_name == "fmnist":
                sensitive_train_set = torchvision.datasets.FashionMNIST(root=data_dir, train=True, download=True, transform=transforms.ToTensor())
                sensitive_test_set = torchvision.datasets.FashionMNIST(root=data_dir, train=False, download=True, transform=transforms.ToTensor())
            elif data_name == "eurosat":
                sensitive_set = torchvision.datasets.ImageFolder(root=os.path.join(data_dir, "EuroSAT_RGB"), transform=transforms.ToTensor())
                torch.manual_seed(0)
                train_size = 23000
                test_size = 4000
                sensitive_train_set, sensitive_test_set = torch.utils.data.random_split(sensitive_set, [train_size, test_size])
            elif data_name == "cifar10":
                sensitive_train_set = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transforms.ToTensor())
                sensitive_test_set = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transforms.ToTensor())
            elif data_name == "cifar100":
                sensitive_train_set = torchvision.datasets.CIFAR100(root=data_dir, train=True, download=True, transform=transforms.ToTensor())
                sensitive_test_set = torchvision.datasets.CIFAR100(root=data_dir, train=False, download=True, transform=transforms.ToTensor())
            elif data_name == "celeba":
                train_name = train_name + '_' + config.celeba_attr
                test_name = test_name + '_' + config.celeba_attr

                with open(os.path.join(data_dir, "celeba", "list_attr_celeba.txt"), 'r') as f:
                    attr_index = f.readlines()[1].strip().split().index(config.celeba_attr)
                target_transform = target_trans(attr_index)
                sensitive_train_set = torchvision.datasets.CelebA(root=data_dir, split="train",  download=False, transform=transforms.ToTensor(), target_transform=target_transform)
                sensitive_test_set = torchvision.datasets.CelebA(root=data_dir, split="test", download=False, transform=transforms.ToTensor(), target_transform=target_transform)
            elif data_name == "camelyon":
                sensitive_train_set = torchvision.datasets.ImageFolder(root=os.path.join(data_dir, "camelyon17_32", "train"), transform=transforms.ToTensor())
                sensitive_test_set = torchvision.datasets.ImageFolder(root=os.path.join(data_dir, "camelyon17_32", "test"), transform=transforms.ToTensor())
            elif data_name == "places365":
                # _ = torchvision.datasets.Places365(root=data_dir, small=True, download=True)
                resize_images(os.path.join(data_dir, "data_256_standard"), os.path.join(data_dir, "data_{}_standard".format(config.resolution)), (config.resolution, config.resolution))
                return
            elif data_name == "emnist":
                _ = torchvision.datasets.EMNIST(root=data_dir, train=True, split="letters", download=True)
                return
            elif data_name == "lsun":
                sensitive_train_set = torchvision.datasets.LSUN(root=data_dir, classes="bedroom_train")
                sensitive_test_set = torchvision.datasets.LSUN(root=data_dir, classes="bedroom_test")
                sensitive_set = torchvision.datasets.ImageFolder(root=config.train_path, transform=transforms.ToTensor())
                torch.manual_seed(0)
                train_size = int(len(sensitive_set) * 0.8)
                test_size = len(sensitive_set) - train_size
                sensitive_train_set, sensitive_test_set = torch.utils.data.random_split(sensitive_set, [train_size, test_size])
            elif data_name == "covidx-cxr4":
                # Load COVIDx CXR-4 dataset and merge train.txt and val.txt
                train_split_file = os.path.join(data_dir, "train.txt")
                val_split_file = os.path.join(data_dir, "val.txt")
                test_split_file = os.path.join(data_dir, "test.txt")
                print(f"Checking train.txt: {train_split_file}")
                print(f"Checking val.txt: {val_split_file}")
                print(f"Checking test.txt: {test_split_file}")
                transform = transforms.Compose([
                    transforms.Resize((config.resolution, config.resolution)), 
                    transforms.ToTensor()
                ])
                # Merge train.txt and val.txt in order
                split_files = []
                if os.path.exists(train_split_file):
                    split_files.append(train_split_file)
                if os.path.exists(val_split_file):
                    split_files.append(val_split_file)
                if not split_files:
                    raise FileNotFoundError("train.txt or val.txt does not exist")
                sensitive_train_set = COVIDxDataset(root_dir=data_dir, split_files=split_files, transform=transform)
                sensitive_test_set = COVIDxDataset(root_dir=data_dir, split_files=[test_split_file], transform=transform)
            elif config.train_path != '' and config.test_path != '':
                sensitive_train_set = torchvision.datasets.ImageFolder(root=config.train_path, transform=transforms.ToTensor())
                sensitive_test_set = torchvision.datasets.ImageFolder(root=config.test_path, transform=transforms.ToTensor())
            else:
                raise NotImplementedError('{} is not yet implemented.'.format(data_name))

            sensitive_train_loader = torch.utils.data.DataLoader(dataset=sensitive_train_set, batch_size=1)
            sensitive_test_loader = torch.utils.data.DataLoader(dataset=sensitive_test_set, batch_size=1)

            if config.max_image is None:
                max_idx = len(sensitive_train_set)
            else:
                max_idx = min(config.max_image, len(sensitive_train_set))

            transform_image = make_transform(config.transform, config.resolution, config.resolution)
            
            if config.max_image is None:
                max_idx = len(sensitive_train_set)
            else:
                max_idx = min(config.max_image, len(sensitive_train_set))
            archive_root_dir, save_bytes, close_dest = open_dest(os.path.join(data_dir, train_name + '.zip'))
            data_save(sensitive_train_loader, max_idx, transform_image, archive_root_dir, save_bytes, close_dest)

            if config.max_image is None:
                max_idx = len(sensitive_test_set)
            else:
                max_idx = min(config.max_image, len(sensitive_test_set))
            archive_root_dir, save_bytes, close_dest = open_dest(os.path.join(data_dir, test_name + '.zip'))
            data_save(sensitive_test_loader, max_idx, transform_image, archive_root_dir, save_bytes, close_dest)

            dataset = ImageFolderDataset(os.path.join(data_dir, train_name + '.zip'))

            # calculate the feature of training image for FID metric
            fid_save(os.path.join(data_dir, fid_name + '.npz'), dataset, config.fid_batch_size)
    


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_name', nargs="*", default=["mnist", "fmnist", "cifar10", "cifar100", "celeba", "camelyon", "imagenet", "places365", "emnist", "lsun",  "covidx-cxr4"], help='List of datasets to use. Default is all provided datasets.')
    parser.add_argument('--resolution', default=32, type=int, help='Resolution of the images. Default is 32.')
    parser.add_argument('--c', default=3, type=int, help='Number of color channels in the images. Default is 3 (RGB).')
    parser.add_argument('--fid_batch_size', default=500, type=int, help='Batch size for FID calculation. Default is 500.')
    parser.add_argument('--data_dir', default='../dataset', help='Directory where the datasets are stored. Default is ../dataset.')
    parser.add_argument('--transform', default="center-crop", help='Type of transform to apply to the data. Default is center-crop.')
    parser.add_argument('--celeba_attr', default="Male", help='Attribute to filter CelebA dataset. Default is Male.')
    parser.add_argument('--max_image', default=None, type=int, help='Maximum number of images to load. Default is None (load all).')
    parser.add_argument('--train_path', default='', help='Path to the custom training set. Leave empty if using default datasets.')
    parser.add_argument('--test_path', default='', help='Path to the custom test set. Leave empty if using default datasets.')
    config = parser.parse_args()
    main(config)
